{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TextKGCData: Textual Knowledge Graph Data Toolkit","text":"<p>This package provides tools for downloading, processing, and standardizing knowledge graph data with textual descriptions. It includes SimKGC-compatible preprocessing for WN18RR, FB15k-237, and Wikidata5M datasets.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#add-to-your-git-project","title":"Add to Your Git Project","text":"<pre><code>git submodule add https://github.com/TJ-coding/TextKGCData.git packages/text-kgc-data\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/TJ-coding/TextKGCData.git@branch#subdirectory=text_kgc_data_proj\n</code></pre>"},{"location":"#command-line-usage","title":"Command Line Usage","text":""},{"location":"#download","title":"Download","text":"<p>All operations are available via the CLI:</p> <pre><code>text-kgc [DATASET] [COMMAND] [OPTIONS]\n</code></pre>"},{"location":"#batch-operations-recommended","title":"Batch Operations (Recommended)","text":"<p>Download and Process All Datasets: <pre><code># Complete pipeline - downloads and processes all datasets\ntext-kgc download-and-process-all\n\n# Or run separately:\ntext-kgc download-all      # Downloads all datasets\ntext-kgc process-all       # Processes all datasets\n</code></pre></p>"},{"location":"#supported-datasets-and-commands","title":"Supported Datasets and Commands","text":"<p>WN18RR Dataset: <pre><code># Download WN18RR dataset\ntext-kgc wn18rr download data/raw/wn18rr\n\n# Process with SimKGC compatibility\ntext-kgc wn18rr process data/raw/wn18rr data/standardised/wn18rr\n</code></pre></p> <p>FB15k-237 Dataset: <pre><code># Download FB15k-237 dataset\ntext-kgc fb15k237 download data/raw/fb15k237\n\n# Process with SimKGC compatibility\ntext-kgc fb15k237 process data/raw/fb15k237 data/standardised/fb15k237\n</code></pre></p> <p>Wikidata5M Dataset: <pre><code># Download transductive variant\ntext-kgc wikidata5m download-transductive data/raw/wikidata5m-transductive\n\n# Download inductive variant  \ntext-kgc wikidata5m download-inductive data/raw/wikidata5m-inductive\n\n# Process transductive variant\ntext-kgc wikidata5m process-transductive data/raw/wikidata5m-transductive data/standardised/wikidata5m-transductive\n\n# Process inductive variant\ntext-kgc wikidata5m process-inductive data/raw/wikidata5m-inductive data/standardised/wikidata5m-inductive\n</code></pre></p>"},{"location":"#python-api","title":"Python API","text":"<p>Load processed knowledge graph files:</p> <pre><code>from text_kgc_data.io import load_standardized_kg\n\n# Load all standardized data at once\nkg_data = load_standardized_kg(\"data/standardised/wn18rr\")\n\n# Access the loaded data\nentity_id2name = kg_data['entities']      # Entity ID -&gt; name mappings\nentity_id2description = kg_data['descriptions']  # Entity ID -&gt; description mappings  \nrelation_id2name = kg_data['relations']   # Relation ID -&gt; name mappings\n</code></pre> <p>Or load individual files:</p> <pre><code>from text_kgc_data.io import load_json\n\n# Load individual files manually\nentity_id2name = load_json(\"data/standardised/&lt;dataset&gt;/entity_id2name.json\")\nentity_id2description = load_json(\"data/standardised/&lt;dataset&gt;/entity_id2description.json\")\nrelation_id2name = load_json(\"data/standardised/&lt;dataset&gt;/relation_id2name.json\")\n</code></pre> <p>Use the truncation utilities directly:</p> <pre><code>from text_kgc_data.truncation import truncate_descriptions, get_truncation_limit\n\n# Dataset-aware truncation\nentity_descs = truncate_descriptions(\n    entity_descriptions,\n    dataset='wn18rr',\n    content_type='entity'\n)\n\n# Check truncation limits\nlimit = get_truncation_limit('fb15k237', 'relation')  # Returns 10\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>SimKGC Compatible: Identical preprocessing to SimKGC paper implementation</li> <li>Dataset-Aware Truncation: Automatic word limits per dataset (WN18RR: 50/30, FB15k-237: 50/10, Wikidata5M: 50/30)</li> <li>Word-Based Processing: Uses word splitting instead of tokenization for consistency</li> <li>Multiple Variants: Supports both transductive and inductive Wikidata5M evaluation settings</li> </ul>"},{"location":"#dataset-specific-guides","title":"\ud83d\udcd6 Dataset-Specific Guides","text":"<p>For detailed processing instructions and academic paper preparation:</p> <ul> <li>WN18RR Processing Guide - WordNet knowledge graph with 40k entities</li> <li>FB15k-237 Processing Guide - Freebase subset with 14k entities  </li> <li>Wikidata5M Processing Guide - Large-scale Wikidata with 5M entities</li> </ul> <p>Each guide includes command references, step-by-step tutorials, and copy-pasteable methods sections for academic papers.</p>"},{"location":"#for-developers","title":"\ud83d\udd27 For Developers","text":"<ul> <li>Adding New Datasets - Complete guide for extending the toolkit with new knowledge graph datasets</li> </ul>"},{"location":"adding_datasets/","title":"Adding New Datasets","text":"<p>This guide explains how to add support for new knowledge graph datasets to the TextKGCData toolkit.</p>"},{"location":"adding_datasets/#overview","title":"Overview","text":"<p>The TextKGCData toolkit follows a functional architecture where each dataset has its own module containing specific processing functions. To add a new dataset, you'll need to:</p> <ol> <li>Create a new dataset module</li> <li>Implement the required functions</li> <li>Add CLI commands</li> <li>Update the main exports</li> </ol>"},{"location":"adding_datasets/#creating-a-dataset-module","title":"Creating a Dataset Module","text":"<p>Create a new file in <code>text_kgc_data/datasets/</code> named after your dataset (e.g., <code>my_dataset.py</code>).</p>"},{"location":"adding_datasets/#required-functions","title":"Required Functions","text":"<p>Each dataset module should implement these core functions:</p> <pre><code>from pathlib import Path\nfrom typing import Dict\nfrom beartype import beartype\n\n@beartype\ndef download_my_dataset(output_dir: Path) -&gt; Path:\n    \"\"\"Download dataset files.\n\n    Args:\n        output_dir: Directory where raw data will be saved\n\n    Returns:\n        Path to the downloaded data directory\n    \"\"\"\n    # Implementation here\n    pass\n\n@beartype\ndef create_entity_id2name_my_dataset(raw_data_dir: Path, output_file: Path) -&gt; Dict[str, str]:\n    \"\"\"Create entity ID to name mapping.\n\n    Args:\n        raw_data_dir: Directory containing raw dataset files\n        output_file: Where to save the mapping JSON file\n\n    Returns:\n        Dictionary mapping entity IDs to names\n    \"\"\"\n    # Implementation here\n    pass\n\n@beartype\ndef create_entity_id2description_my_dataset(raw_data_dir: Path, output_file: Path) -&gt; Dict[str, str]:\n    \"\"\"Create entity ID to description mapping.\n\n    Args:\n        raw_data_dir: Directory containing raw dataset files\n        output_file: Where to save the mapping JSON file\n\n    Returns:\n        Dictionary mapping entity IDs to descriptions\n    \"\"\"\n    # Implementation here\n    pass\n\n@beartype\ndef create_relation_id2name_my_dataset(raw_data_dir: Path, output_file: Path) -&gt; Dict[str, str]:\n    \"\"\"Create relation ID to name mapping.\n\n    Args:\n        raw_data_dir: Directory containing raw dataset files\n        output_file: Where to save the mapping JSON file\n\n    Returns:\n        Dictionary mapping relation IDs to names\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"adding_datasets/#function-naming-convention","title":"Function Naming Convention","text":"<p>Functions should be named to clearly describe what they create:</p> <ul> <li><code>create_entity_id2name_*</code> - Creates entity ID to name mappings</li> <li><code>create_entity_id2description_*</code> - Creates entity ID to description mappings</li> <li><code>create_relation_id2name_*</code> - Creates relation ID to name mappings</li> <li><code>download_*</code> - Downloads raw dataset files</li> </ul>"},{"location":"adding_datasets/#adding-cli-commands","title":"Adding CLI Commands","text":"<p>Update <code>text_kgc_data/cli.py</code> to add your dataset commands:</p> <pre><code># Add imports\nfrom text_kgc_data.datasets.my_dataset import (\n    download_my_dataset,\n    create_entity_id2name_my_dataset,\n    create_entity_id2description_my_dataset,\n    create_relation_id2name_my_dataset,\n)\n\n# Create subcommand app\nmy_dataset_app = typer.Typer(help=\"My Dataset processing commands\")\n\n@my_dataset_app.command(\"download\")\n@beartype\ndef download_my_dataset_cli(\n    output_dir: Path = typer.Argument(..., help=\"Output directory for raw data\"),\n) -&gt; None:\n    \"\"\"Download My Dataset files.\"\"\"\n    result_dir = download_my_dataset(output_dir)\n    typer.echo(f\"Downloaded My Dataset to: {result_dir}\")\n\n# Add more commands...\n\n# Register with main app\napp.add_typer(my_dataset_app, name=\"my-dataset\")\n</code></pre>"},{"location":"adding_datasets/#updating-exports","title":"Updating Exports","text":"<p>Add your functions to <code>text_kgc_data/__init__.py</code>:</p> <pre><code># Add to imports\nfrom text_kgc_data.datasets.my_dataset import (\n    download_my_dataset,\n    create_entity_id2name_my_dataset,\n    create_entity_id2description_my_dataset,\n    create_relation_id2name_my_dataset,\n)\n\n# Add to __all__\n__all__ = [\n    # ... existing exports ...\n    \"download_my_dataset\",\n    \"create_entity_id2name_my_dataset\", \n    \"create_entity_id2description_my_dataset\",\n    \"create_relation_id2name_my_dataset\",\n]\n</code></pre>"},{"location":"adding_datasets/#testing-your-dataset","title":"Testing Your Dataset","text":"<p>You can test your dataset functions both programmatically and via CLI:</p> <pre><code># Programmatic usage\nfrom text_kgc_data import download_my_dataset, create_entity_id2name_my_dataset\n\ndata_dir = download_my_dataset(Path(\"./data\"))\nmappings = create_entity_id2name_my_dataset(data_dir, Path(\"./mappings.json\"))\n</code></pre> <pre><code># CLI usage  \ntext-kgc my-dataset download ./data\ntext-kgc my-dataset create-entity-mappings ./data ./mappings.json\n</code></pre>"},{"location":"adding_datasets/#example-existing-datasets","title":"Example: Existing Datasets","text":"<p>Look at <code>datasets/wn18rr.py</code> and <code>datasets/wikidata5m.py</code> for complete examples of dataset implementations following this pattern.</p>"},{"location":"fb15k237_example/","title":"FB15k-237 Processing Guide","text":""},{"location":"fb15k237_example/#command-reference","title":"Command Reference","text":"Command Purpose <code>text-kgc fb15k237 download</code> Download raw FB15k-237 dataset <code>text-kgc fb15k237 process</code> Complete SimKGC-compatible processing <code>text-kgc fb15k237 create-entity-text</code> Create entity name/description mappings <code>text-kgc fb15k237 create-relation-text</code> Create relation name mappings <code>text-kgc fb15k237 process-pipeline</code> Complete pipeline with options <code>text-kgc fb15k237 fill-missing-entries</code> Fill missing entity entries <code>text-kgc fb15k237 truncate-descriptions</code> Truncate descriptions to word limit"},{"location":"fb15k237_example/#quick-start","title":"Quick Start","text":"<p>Single Dataset (FB15k-237 only): <pre><code>text-kgc fb15k237 download data/raw/fb15k237\ntext-kgc fb15k237 process data/raw/fb15k237 data/standardised/fb15k237\n</code></pre></p> <p>All Datasets (Recommended): <pre><code>text-kgc download-and-process-all\n</code></pre></p>"},{"location":"fb15k237_example/#step-by-step-processing","title":"Step-by-Step Processing","text":"<p>1. Download Dataset <pre><code>text-kgc fb15k237 download data/raw/fb15k237\n</code></pre></p> <p>2. Create Entity Text <pre><code>text-kgc fb15k237 create-entity-text \\\n  data/raw/fb15k237/FB15k_mid2description.txt \\\n  data/standardised/fb15k237\n</code></pre></p> <p>3. Create Relation Text <pre><code>text-kgc fb15k237 create-relation-text \\\n  data/raw/fb15k237/relations.dict \\\n  data/standardised/fb15k237\n</code></pre></p> <p>4. Pipeline (Alternative) <pre><code>text-kgc fb15k237 process-pipeline \\\n  data/raw/fb15k237 \\\n  data/standardised/fb15k237 \\\n  --fill-missing \\\n  --truncate-descriptions \\\n  --max-words 50\n</code></pre></p>"},{"location":"fb15k237_example/#python-usage","title":"Python Usage","text":"<pre><code>from text_kgc_data.tkg_io import load_tkg_from_files\n\ntextual_fb15k237_kg = load_tkg_from_files(\n    \"data/standardised/fb15k237/entity_id2name.json\",\n    \"data/standardised/fb15k237/entity_id2description.json\", \n    \"data/standardised/fb15k237/relation_id2name.json\"\n)\n</code></pre>"},{"location":"fb15k237_example/#preprocessing-details-for-academic-papers","title":"Preprocessing Details for Academic Papers","text":""},{"location":"fb15k237_example/#fb15k-237-dataset-specification","title":"FB15k-237 Dataset Specification","text":"<ul> <li>Source: Freebase Knowledge Graph (filtered subset)</li> <li>Entities: 14,541 unique entities</li> <li>Relations: 237 semantic relations  </li> <li>Splits: 272,115 train / 17,535 validation / 20,466 test triplets</li> </ul>"},{"location":"fb15k237_example/#text-processing-methodology","title":"Text Processing Methodology","text":"<p>Entity Name Cleaning: - Removes namespace prefixes from Freebase entity identifiers - Converts underscores to spaces for readability - Example transformation: <code>/m/02mjmr</code> \u2192 entity name from mid2name mapping</p> <p>Relation Name Processing: - Removes namespace prefixes (e.g., <code>/base/</code>, <code>/people/</code>) - Converts forward slashes to spaces - Deduplicates consecutive identical tokens - Example transformation: <code>/people/person/nationality</code> \u2192 <code>nationality person people</code></p> <p>Text Truncation: - Method: Word-based truncation (not subword tokenization) - Implementation: <code>text.split()[:max_words]</code> followed by <code>' '.join()</code> - Entity descriptions: 50 words maximum - Relation descriptions: 10 words maximum (FB15k-237 specific) - Rationale: Ensures consistent text lengths across tokenizers</p> <p>Missing Data Handling: - Strategy: Empty string (<code>''</code>) for missing descriptions - No artificial placeholder tokens introduced - Maintains data structure consistency</p> <p>Text Sources: - Entity descriptions: <code>FB15k_mid2description.txt</code> - Entity names: <code>FB15k_mid2name.txt</code> - Relation names: Derived from relation identifiers with cleaning</p> <p>Technical Specifications: - Character encoding: UTF-8 - Tokenizer compatibility: BERT-base-uncased (default) - Output format: Standardized JSON mappings + plain text entity lists - SimKGC compatibility: Full preprocessing pipeline alignment</p> <p>Citation Notes: This preprocessing follows SimKGC methodology (Wang et al., 2022). Word-based truncation ensures reproducibility across different tokenization schemes. For academic use, specify: \"FB15k-237 entity descriptions truncated to 50 words, relation descriptions to 10 words using word-based splitting.\"</p>"},{"location":"fb15k237_example/#paper-ready-summary","title":"Paper-Ready Summary","text":"<p>Copy-paste for Methods section:</p> <p>FB15k-237 Dataset Preprocessing: We process the FB15k-237 dataset using SimKGC-compatible preprocessing following Wang et al. (2022). The dataset contains 14,541 entities and 237 relations with 272,115/17,535/20,466 train/validation/test triplets. Entity names and descriptions are sourced from Freebase mid-to-name and mid-to-description mappings. Entity descriptions are truncated to 50 words and relation names to 10 words using word-based splitting. Relation names undergo namespace cleaning by removing prefixes like <code>/people/</code> and converting forward slashes to spaces. Missing descriptions are represented as empty strings to maintain consistent data structure.</p>"},{"location":"standardised_tkg/","title":"Standardised File Format","text":"<p>TextKGCData uses a standardised file format for knowledge graph data with textual descriptions.</p>"},{"location":"standardised_tkg/#file-types","title":"File Types","text":""},{"location":"standardised_tkg/#entity_idstxt","title":"<code>entity_ids.txt</code>","text":"<p>Plain text file with one entity ID per line. <pre><code>Q42\nQ123\nQ999\n</code></pre></p>"},{"location":"standardised_tkg/#entity_id2_namejson","title":"<code>entity_id2_name.json</code>","text":"<p>JSON mapping entity IDs to names. <pre><code>{\n  \"Q42\": \"Douglas Adams\",\n  \"Q123\": \"Example Entity\"\n}\n</code></pre></p>"},{"location":"standardised_tkg/#entity_id2_descriptionjson","title":"<code>entity_id2_description.json</code>","text":"<p>JSON mapping entity IDs to descriptions. <pre><code>{\n  \"Q42\": \"Douglas Adams was an English author, best known for The Hitchhiker's Guide to the Galaxy.\",\n  \"Q123\": \"This is a sample entity used for demonstration purposes.\"\n}\n</code></pre></p>"},{"location":"standardised_tkg/#relation_id2namejson","title":"<code>relation_id2name.json</code>","text":"<p>JSON mapping relation IDs to names. <pre><code>{\n  \"P31\": \"instance of\",\n  \"P279\": \"subclass of\",\n  \"P50\": \"author\"\n}\n</code></pre></p>"},{"location":"standardised_tkg/#directory-structure","title":"Directory Structure","text":"<pre><code>&lt;dataset&gt;_standardized_/\n    entity_ids.txt\n    entity_id2_name.json\n    entity_id2_description.json\n    relation_id2name.json\n</code></pre>"},{"location":"wikidata5m_example/","title":"Wikidata5M Processing Guide","text":""},{"location":"wikidata5m_example/#command-reference","title":"Command Reference","text":"Command Purpose <code>text-kgc wikidata5m download-transductive</code> Download transductive evaluation data <code>text-kgc wikidata5m download-inductive</code> Download inductive evaluation data <code>text-kgc wikidata5m process-transductive</code> Process transductive variant <code>text-kgc wikidata5m process-inductive</code> Process inductive variant <code>text-kgc wikidata5m create-entity-text</code> Create entity name/description mappings <code>text-kgc wikidata5m create-relation-text</code> Create relation name mappings <code>text-kgc wikidata5m process-pipeline</code> Complete pipeline with options <code>text-kgc wikidata5m fill-missing-entries</code> Fill missing entity entries <code>text-kgc wikidata5m truncate-descriptions</code> Truncate descriptions to word limit"},{"location":"wikidata5m_example/#quick-start","title":"Quick Start","text":"<p>Single Variants:</p> <p>Transductive Setting: <pre><code>text-kgc wikidata5m download-transductive data/raw/wikidata5m-transductive\ntext-kgc wikidata5m process-transductive data/raw/wikidata5m-transductive data/standardised/wikidata5m-transductive\n</code></pre></p> <p>Inductive Setting: <pre><code>text-kgc wikidata5m download-inductive data/raw/wikidata5m-inductive\ntext-kgc wikidata5m process-inductive data/raw/wikidata5m-inductive data/standardised/wikidata5m-inductive\n</code></pre></p> <p>All Datasets (Recommended): <pre><code>text-kgc download-and-process-all\n</code></pre></p>"},{"location":"wikidata5m_example/#step-by-step-processing","title":"Step-by-Step Processing","text":""},{"location":"wikidata5m_example/#transductive-evaluation","title":"Transductive Evaluation","text":"<p>1. Download Transductive Dataset <pre><code>text-kgc wikidata5m download-transductive data/raw/wikidata5m-transductive\n</code></pre></p> <p>2. Process Transductive Variant <pre><code>text-kgc wikidata5m process-transductive \\\n  data/raw/wikidata5m-transductive \\\n  data/standardised/wikidata5m-transductive\n</code></pre></p> <p>3. Pipeline (Alternative) <pre><code>text-kgc wikidata5m process-pipeline \\\n  data/raw/wikidata5m-transductive \\\n  data/standardised/wikidata5m-transductive \\\n  --variant transductive \\\n  --fill-missing \\\n  --truncate-descriptions \\\n  --max-words 50\n</code></pre></p>"},{"location":"wikidata5m_example/#inductive-evaluation","title":"Inductive Evaluation","text":"<p>1. Download Inductive Dataset <pre><code>text-kgc wikidata5m download-inductive data/raw/wikidata5m-inductive\n</code></pre></p> <p>2. Process Inductive Variant <pre><code>text-kgc wikidata5m process-inductive \\\n  data/raw/wikidata5m-inductive \\\n  data/standardised/wikidata5m-inductive\n</code></pre></p> <p>3. Pipeline (Alternative) <pre><code>text-kgc wikidata5m process-pipeline \\\n  data/raw/wikidata5m-inductive \\\n  data/standardised/wikidata5m-inductive \\\n  --variant inductive \\\n  --fill-missing \\\n  --truncate-descriptions \\\n  --max-words 50\n</code></pre></p>"},{"location":"wikidata5m_example/#python-usage","title":"Python Usage","text":"<p>Transductive: <pre><code>from text_kgc_data.tkg_io import load_tkg_from_files\n\ntextual_wikidata5m_trans = load_tkg_from_files(\n    \"data/standardised/wikidata5m-transductive/entity_id2name.json\",\n    \"data/standardised/wikidata5m-transductive/entity_id2description.json\", \n    \"data/standardised/wikidata5m-transductive/relation_id2name.json\"\n)\n</code></pre></p> <p>Inductive: <pre><code>from text_kgc_data.tkg_io import load_tkg_from_files\n\ntextual_wikidata5m_ind = load_tkg_from_files(\n    \"data/standardised/wikidata5m-inductive/entity_id2name.json\",\n    \"data/standardised/wikidata5m-inductive/entity_id2description.json\", \n    \"data/standardised/wikidata5m-inductive/relation_id2name.json\"\n)\n</code></pre></p>"},{"location":"wikidata5m_example/#preprocessing-details-for-academic-papers","title":"Preprocessing Details for Academic Papers","text":""},{"location":"wikidata5m_example/#wikidata5m-dataset-specification","title":"Wikidata5M Dataset Specification","text":"<ul> <li>Source: Wikidata Knowledge Graph (subset)</li> <li>Entities: ~5 million entities total</li> <li>Relations: 822 semantic relations</li> <li>Evaluation Settings: </li> <li>Transductive: 4,594,485 train / 23,298 validation / 23,357 test triplets</li> <li>Inductive: Disjoint entity sets between train and test</li> </ul>"},{"location":"wikidata5m_example/#text-processing-methodology","title":"Text Processing Methodology","text":"<p>Entity Name Processing: - Uses Wikidata entity labels as primary names - Truncates entity names to 10 words maximum - Handles multilingual labels (English preference) - Example: <code>Q42</code> \u2192 <code>Douglas Adams</code></p> <p>Entity Description Processing: - Sources descriptions from Wikidata entity descriptions - Truncates to 50 words maximum using word-based splitting - Maintains original description quality from Wikidata</p> <p>Relation Name Processing: - Converts Wikidata property identifiers to human-readable names - Truncates relation names to 30 words maximum - Example: <code>P31</code> \u2192 <code>instance of</code></p> <p>Text Truncation: - Method: Word-based truncation (not subword tokenization) - Implementation: <code>text.split()[:max_words]</code> followed by <code>' '.join()</code> - Entity descriptions: 50 words maximum - Relation descriptions: 30 words maximum - Entity names: 10 words maximum (Wikidata5M specific) - Rationale: Ensures consistent text lengths across tokenizers</p> <p>Missing Data Handling: - Strategy: Empty string (<code>''</code>) for missing descriptions - No artificial placeholder tokens introduced - Maintains data structure consistency</p> <p>Evaluation Settings: - Transductive: All entities in train/validation/test are from the same set - Inductive: Test entities are disjoint from training entities - Both settings use identical preprocessing methodology</p> <p>Text Sources: - Entity descriptions: Wikidata entity descriptions - Entity names: Wikidata entity labels - Relation names: Wikidata property labels</p> <p>Technical Specifications: - Character encoding: UTF-8 - Tokenizer compatibility: BERT-base-uncased (default) - Output format: Standardized JSON mappings + plain text entity lists - SimKGC compatibility: Full preprocessing pipeline alignment</p> <p>Citation Notes: This preprocessing follows SimKGC methodology (Wang et al., 2022). Word-based truncation ensures reproducibility across different tokenization schemes. For academic use, specify: \"Wikidata5M entity descriptions truncated to 50 words, relation descriptions to 30 words, entity names to 10 words using word-based splitting.\"</p>"},{"location":"wikidata5m_example/#paper-ready-summary","title":"Paper-Ready Summary","text":"<p>Copy-paste for Methods section:</p> <p>Wikidata5M Dataset Preprocessing: We process the Wikidata5M dataset using SimKGC-compatible preprocessing following Wang et al. (2022). The dataset supports both transductive and inductive evaluation settings with 4,594,485/23,298/23,357 train/validation/test triplets for transductive evaluation. Entity names and descriptions are sourced from Wikidata labels and descriptions respectively. Entity descriptions are truncated to 50 words, relation names to 30 words, and entity names to 10 words using word-based splitting. The inductive setting uses disjoint entity sets between training and test data to evaluate generalization to unseen entities. Missing descriptions are represented as empty strings to maintain consistent data structure.</p>"},{"location":"wikidata5m_example/#4-fill-missing-entity-namesdescriptions-optional","title":"4. Fill Missing Entity Names/Descriptions (Optional)","text":"<p>If you want to ensure that every entity has both a name and a description, fill missing entries with a placeholder:</p> <pre><code>tkg fill-missing-entries-cli \\\n  --entity-id2name-source-path wikidata5m_tkg/entity_id2name.json \\\n  --entity-id2description-source-path wikidata5m_tkg/entity_id2description.json \\\n  --entity-id2name-save-path wikidata5m_tkg/filled_entity_id2name.json \\\n  --entity-id2description-save-path wikidata5m_tkg/filled_entity_id2description.json \\\n  --place-holder-character \"-\"\n</code></pre>"},{"location":"wikidata5m_example/#5-truncate-descriptions-optional-for-model-compatibility","title":"5. Truncate Descriptions (Optional, for model compatibility)","text":"<p>To ensure descriptions fit within a model's token limit (e.g., 50 tokens for SimKGC), run:</p> <pre><code>tkg truncate-description-cli \\\n  gpt2 \\\n  --entity-id2description-source-path wikidata5m_tkg/filled_entity_id2description.json \\\n  --entity-id2description-save-path wikidata5m_tkg/truncated_entity_id2description.json \\\n  --truncate-tokens 50\n</code></pre>"},{"location":"wikidata5m_example/#6-load-the-processed-data-in-python","title":"6. Load the Processed Data in Python","text":"<p>You can now load the processed Wikidata5M files using the <code>load_tkg_from_files</code> utility:</p> <pre><code>from deer_dataset_manager.tkg_io import load_tkg_from_files\n\nentity_id2name_source_path = \"wikidata5m_tkg/filled_entity_id2name.json\"  # Dict[str, str]\nentity_id2description_source_path = \"wikidata5m_tkg/truncated_entity_id2description.json\" # Dict[str, str]\nrelation_id2name_source_path = \"wikidata5m_tkg/relation_id2name.json\" # Dict[str, str]\n\ntextual_wikidata5m_kg = load_tkg_from_files(\n    entity_id2name_source_path,\n    entity_id2description_source_path,\n    relation_id2name_source_path,\n)\n</code></pre>"},{"location":"wikidata5m_example/#notes","title":"Notes","text":"<ul> <li>It picks the first name out of list of names provided in the original Wikidata5M files for the entity names as well as relation names.</li> <li>All CLI commands support custom input/output paths for flexible workflows.</li> <li>You can skip steps 4 and 5 if your data is already complete and within token limits.</li> <li>For more details, see the main documentation or run <code>tkg --help</code>.</li> </ul>"},{"location":"wn18rr_example/","title":"WN18RR Processing Guide","text":""},{"location":"wn18rr_example/#command-reference","title":"Command Reference","text":"Command Purpose <code>text-kgc wn18rr download</code> Download raw WN18RR dataset <code>text-kgc wn18rr process</code> Complete SimKGC-compatible processing <code>text-kgc wn18rr create-entity-text</code> Create entity name/description mappings <code>text-kgc wn18rr create-relation-text</code> Create relation name mappings <code>text-kgc wn18rr process-pipeline</code> Complete pipeline with options <code>text-kgc wn18rr fill-missing-entries</code> Fill missing entity entries <code>text-kgc wn18rr truncate-descriptions</code> Truncate descriptions to word limit"},{"location":"wn18rr_example/#batch-commands-all-datasets","title":"Batch Commands (All Datasets)","text":"Command Purpose <code>text-kgc download-all</code> Download all datasets (WN18RR + FB15k-237 + Wikidata5M) <code>text-kgc process-all</code> Process all datasets with SimKGC compatibility <code>text-kgc download-and-process-all</code> Complete pipeline for all datasets"},{"location":"wn18rr_example/#quick-start","title":"Quick Start","text":"<p>Single Dataset (WN18RR only): <pre><code>text-kgc wn18rr download data/raw/wn18rr\ntext-kgc wn18rr process data/raw/wn18rr data/standardised/wn18rr\n</code></pre></p> <p>All Datasets (Recommended): <pre><code>text-kgc download-and-process-all\n</code></pre></p>"},{"location":"wn18rr_example/#step-by-step-processing","title":"Step-by-Step Processing","text":"<p>1. Download Dataset <pre><code>text-kgc wn18rr download data/raw/wn18rr\n</code></pre></p> <p>2. Create Entity Text <pre><code>text-kgc wn18rr create-entity-text \\\n  data/raw/wn18rr/wordnet-mlj12-definitions.txt \\\n  data/standardised/wn18rr\n</code></pre></p> <p>3. Create Relation Text <pre><code>text-kgc wn18rr create-relation-text \\\n  data/raw/wn18rr/relations.dict \\\n  data/standardised/wn18rr\n</code></pre></p> <p>4. Pipeline (Alternative) <pre><code>text-kgc wn18rr process-pipeline \\\n  data/raw/wn18rr \\\n  data/standardised/wn18rr \\\n  --fill-missing \\\n  --truncate-descriptions \\\n  --max-words 50\n</code></pre></p>"},{"location":"wn18rr_example/#python-usage","title":"Python Usage","text":"<pre><code>from text_kgc_data.io import load_standardized_kg\n\n# Load all WN18RR data at once\nwn18rr_data = load_standardized_kg(\"data/standardised/wn18rr\")\n\n# Access the data\nentities = wn18rr_data['entities']          # Entity ID -&gt; name  \ndescriptions = wn18rr_data['descriptions']  # Entity ID -&gt; description\nrelations = wn18rr_data['relations']        # Relation ID -&gt; name\n</code></pre> <p>Or load individual files:</p> <pre><code>from text_kgc_data.io import load_json\n\n# Load individual files manually\nentity_id2name = load_json(\"data/standardised/wn18rr/entity_id2name.json\")\nentity_id2description = load_json(\"data/standardised/wn18rr/entity_id2description.json\")\nrelation_id2name = load_json(\"data/standardised/wn18rr/relation_id2name.json\")\n</code></pre>"},{"location":"wn18rr_example/#preprocessing-details-for-academic-papers","title":"Preprocessing Details for Academic Papers","text":""},{"location":"wn18rr_example/#wn18rr-dataset-specification","title":"WN18RR Dataset Specification","text":"<ul> <li>Source: WordNet 18 - Reduced Relations</li> <li>Entities: 40,943 unique synsets</li> <li>Relations: 11 semantic relations  </li> <li>Splits: 86,835 train / 3,034 validation / 3,134 test triplets</li> </ul>"},{"location":"wn18rr_example/#text-processing-methodology","title":"Text Processing Methodology","text":"<p>Entity Name Cleaning: - Removes <code>__</code> prefix from WordNet synset identifiers - Strips POS tags and sense numbers (e.g., <code>_NN_1</code> suffix) - Example transformation: <code>__dog_NN_1</code> \u2192 <code>dog</code></p> <p>Text Truncation: - Method: Word-based truncation (not subword tokenization) - Implementation: <code>text.split()[:max_words]</code> followed by <code>' '.join()</code> - Entity descriptions: 50 words maximum - Relation descriptions: 30 words maximum - Rationale: Ensures consistent text lengths across tokenizers</p> <p>Missing Data Handling: - Strategy: Empty string (<code>''</code>) for missing descriptions - No artificial placeholder tokens introduced - Maintains data structure consistency</p> <p>Text Sources: - Entity descriptions: <code>wordnet-mlj12-definitions.txt</code> - Entity names: Derived from synset identifiers after cleaning - Relation names: <code>relations.dict</code> with underscore-to-space conversion</p> <p>Technical Specifications: - Character encoding: UTF-8 - Tokenizer compatibility: BERT-base-uncased (default) - Output format: Standardized JSON mappings + plain text entity lists - SimKGC compatibility: Full preprocessing pipeline alignment</p> <p>Citation Notes: This preprocessing follows SimKGC methodology (Wang et al., 2022). Word-based truncation ensures reproducibility across different tokenization schemes. For academic use, specify: \"WN18RR entity descriptions truncated to 50 words, relation descriptions to 30 words using word-based splitting.\"</p>"},{"location":"wn18rr_example/#paper-ready-summary","title":"Paper-Ready Summary","text":"<p>Copy-paste for Methods section:</p> <p>WN18RR Dataset Preprocessing: We process the WN18RR dataset using SimKGC-compatible preprocessing following Wang et al. (2022). The dataset contains 40,943 entities and 11 relations with 86,835/3,034/3,134 train/validation/test triplets. Entity names are derived from WordNet synset identifiers by removing the <code>__</code> prefix and POS tag suffixes (e.g., <code>__dog_NN_1</code> \u2192 <code>dog</code>). Entity descriptions are sourced from the WordNet-MLJ12 definitions provided by Yao et al. (2019) (https://arxiv.org/abs/1909.03193) and truncated to 50 words using word-based splitting. Relation names are truncated to 30 words with underscores converted to spaces. Missing descriptions are represented as empty strings to maintain consistent data structure.</p>"},{"location":"blog/","title":"Blog","text":""}]}
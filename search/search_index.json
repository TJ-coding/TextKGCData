{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TextKGCData: Textual Knowledge Graph Data Toolkit","text":"<p>This package provides tools for downloading, processing, standardizing, and loading knowledge graph data with textual descriptions. It includes a command-line interface (CLI) for all major data preparation and preprocessing steps.</p>"},{"location":"#add-to-your-project","title":"Add to Your Project","text":"<pre><code>git submodule add https://github.com/TJ-coding/TextKGCData.git packages\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/TJ-coding/TextKGCData.git@branch#subdirectory=text_kgc_data_proj\n</code></pre>"},{"location":"#cli-commands","title":"CLI Commands","text":"<p>All commands are available via the CLI defined in <code>text_kgc_data/cli.py</code>. Example usage:</p> <pre><code>python -m text_kgc_data.cli [COMMAND] [OPTIONS]\n</code></pre>"},{"location":"#download-data","title":"Download Data","text":"<ul> <li>download_text_kgc_dataset</li> </ul> <p>Download the text-based KGC dataset from the SimKGC repository.</p> <pre><code>python -m text_kgc_data.cli download-text-kgc-dataset --data-dir-name &lt;output_dir&gt;\n</code></pre>"},{"location":"#standardize-wn18rr-data","title":"Standardize WN18RR Data","text":"<ul> <li>standardize_wn18rr_entity_files_cli</li> </ul> <p>Standardize WN18RR entity files (IDs, names, descriptions).</p> <pre><code>python -m text_kgc_data.cli standardize-wn18rr-entity-files-cli \\\n  --definitions-source-path WN18RR/wordnet-mlj12-definitions.txt \\\n  --entity-id-save-path wn18rr_tkg/entity_ids.txt \\\n  --entity-id2name-save-path wn18rr_tkg/entity_id2_name.txt \\\n  --entity-id2description-save-path wn18rr_tkg/entity_id2_description.txt\n</code></pre> <ul> <li>standardize_wn18rr_relation_file_cli</li> </ul> <p>Standardize WN18RR relation file (relation IDs to descriptions).</p> <pre><code>python -m text_kgc_data.cli standardize-wn18rr-relation-file-cli \\\n  --relations-source-path WN18RR/relations.dict \\\n  --relation-id2name-save-path wn18rr_tkg/wn18rr-relations2description.json\n</code></pre>"},{"location":"#standardize-wikidata5m-data","title":"Standardize Wikidata5M Data","text":"<ul> <li>standardize_wikidata5m_entity_files_cli</li> </ul> <p>Standardize Wikidata5M entity files (IDs, names, descriptions).</p> <pre><code>python -m text_kgc_data.cli standardize-wikidata5m-entity-files-cli \\\n  --entity-names-source-path wikidata5m/wikidata5m_entity.txt \\\n  --entity-descriptions-source-path wikidata5m/wikidata5m_text.txt \\\n  --entity-id-save-path wikidata5m_tkg/entity_ids.txt \\\n  --entity-id2name-save-path wikidata5m_tkg/entity_id2_name.json \\\n  --entity-id2description-save-path wikidata5m_tkg/entity_id2_description.json\n</code></pre> <ul> <li>standardize_wikidata5m_relation_file_cli</li> </ul> <p>Standardize Wikidata5M relation file (relation IDs to names).</p> <pre><code>python -m text_kgc_data.cli standardize-wikidata5m-relation-file-cli \\\n  --relations-source-path wikidata5m/wikidata5m_relation.txt \\\n  --relation-id2name-save-path wikidata5m_tkg/relation_id2name.json\n</code></pre>"},{"location":"#preprocessing-utilities","title":"Preprocessing Utilities","text":"<ul> <li>fill_missing_entries_cli</li> </ul> <p>Fill missing entries in entity name/description JSON files with a placeholder.</p> <pre><code>python -m text_kgc_data.cli fill-missing-entries-cli \\\n  --entity-id2name-path &lt;input_name_json&gt; \\\n  --entity-id2description-path &lt;input_desc_json&gt; \\\n  --output-entity-id2name-path &lt;output_name_json&gt; \\\n  --output-entity-id2description-path &lt;output_desc_json&gt; \\\n  --place-holder-character \"-\"\n</code></pre> <ul> <li>truncate_description_cli</li> </ul> <p>Truncate entity descriptions to a maximum number of tokens using a HuggingFace tokenizer.</p> <pre><code>python -m text_kgc_data.cli truncate-description-cli \\\n  --entity-id2description-path &lt;input_desc_json&gt; \\\n  --output-entity-id2description-path &lt;output_desc_json&gt; \\\n  --tokenizer-name &lt;hf_tokenizer_name&gt; \\\n  --truncate-tokens 50 \\\n  --batch-size 50000\n</code></pre>"},{"location":"#project-layout","title":"Project Layout","text":"<pre><code>\ud83d\udcc1 text_kgc_data/\n\u251c\u2500\u2500 \ud83d\udcc4 cli.py              # Command line interface for all data operations\n\u251c\u2500\u2500 \ud83d\udcc4 download_data.py    # Downloading data from SimKGC Repo\n\u251c\u2500\u2500 \ud83d\udcc4 helpers.py          # Helper functions for TSV/JSON handling\n\u251c\u2500\u2500 \ud83d\udcc4 preprocessors.py    # Data cleaning: fill missing, truncate descriptions\n\u2514\u2500\u2500 \ud83d\udcc1 standardise_tkg_files/            \n    \u251c\u2500\u2500 \ud83d\udcc4 standardise_wn18rr.py      # Standardize WN18RR dataset\n    \u2514\u2500\u2500 \ud83d\udcc4 standardise_wikidata5m.py  # Standardize Wikidata5M dataset\n\ud83d\udcc1 text-kgc-data-docs/\n\u251c\u2500\u2500 \ud83d\udcc4 mkdocs.yml    # MkDocs configuration\n\u2514\u2500\u2500 \ud83d\udcc1 docs/\n    \u251c\u2500\u2500 \ud83d\udcc4 index.md  # Documentation homepage\n    \u2514\u2500\u2500 \ud83d\udcc4 ...       # Other markdown pages, images, files\n</code></pre>"},{"location":"#loading-textual-kg-files-in-python","title":"Loading Textual KG Files in Python","text":"<p>You can load processed textual knowledge graph files using the <code>SimKGCDataLoader</code>:</p> <pre><code>from deer_dataset_manager.tkg_io import load_tkg_from_files\n\nentity_id2name_source_path = \"path/to/entity_id2name.json\"  # Dict[str, str]\nentity_id2description_source_path = \"path/to/entity_id2description.json\" # Dict[str, str]\nrelation_id2name_source_path = \"path/to/relation_id2name.json\" # Dict[str, str]\n\ntextual_kg = load_tkg_from_files(\n    entity_id2name_source_path,\n    entity_id2description_source_path,\n    relation_id2name_source_path,\n)\n</code></pre>"},{"location":"#notes","title":"Notes","text":"<ul> <li>All CLI commands support custom input/output paths for flexible workflows.</li> <li>Preprocessing utilities help ensure data consistency and compatibility with downstream models.</li> <li>See the code in <code>cli.py</code> for the latest available commands and options.</li> </ul>"},{"location":"standardised_tkg/","title":"Standardised Files for Text-Based Knowledge Graph Data","text":"<p>A \"standardised file\" format is used in this project to store knowledge graph (KG) data with rich textual information in a way that is easy to process, share, and load into models. This format ensures consistency across datasets and tools, making it simple to use different KGs with the same codebase.</p>"},{"location":"standardised_tkg/#why-standardise","title":"Why Standardise?","text":"<ul> <li>Interoperability: Use the same loader and processing code for different datasets (e.g., WN18RR, Wikidata5M).</li> <li>Clarity: Each file has a clear, single purpose (IDs, names, descriptions, relations).</li> <li>Extensibility: Easy to add new datasets or fields.</li> </ul>"},{"location":"standardised_tkg/#standardised-file-types","title":"Standardised File Types","text":""},{"location":"standardised_tkg/#1-entity_idstxt","title":"1. <code>entity_ids.txt</code>","text":"<p>A plain text file with one entity ID per line.</p> <p>Example: <pre><code>Q42\nQ123\nQ999\n</code></pre></p>"},{"location":"standardised_tkg/#2-entity_id2_namejson","title":"2. <code>entity_id2_name.json</code>","text":"<p>A JSON dictionary mapping each entity ID to its canonical name.</p> <p>Example: <pre><code>{\n  \"Q42\": \"Douglas Adams\",\n  \"Q123\": \"Example Entity\",\n  \"Q999\": \"Another Entity\"\n}\n</code></pre></p>"},{"location":"standardised_tkg/#3-entity_id2_descriptionjson","title":"3. <code>entity_id2_description.json</code>","text":"<p>A JSON dictionary mapping each entity ID to a textual description.</p> <p>Example: <pre><code>{\n  \"Q42\": \"Douglas Adams was an English author, best known for The Hitchhiker's Guide to the Galaxy.\",\n  \"Q123\": \"This is a sample entity used for demonstration purposes.\",\n  \"Q999\": \"A placeholder entity in the knowledge graph.\"\n}\n</code></pre></p>"},{"location":"standardised_tkg/#4-relation_id2namejson-or-relation_id2descriptionjson","title":"4. <code>relation_id2name.json</code> (or <code>relation_id2description.json</code>)","text":"<p>A JSON dictionary mapping each relation ID to its name or description.</p> <p>Example: <pre><code>{\n  \"P31\": \"instance of\",\n  \"P279\": \"subclass of\",\n  \"P50\": \"author\"\n}\n</code></pre></p>"},{"location":"standardised_tkg/#typical-directory-structure","title":"Typical Directory Structure","text":"<pre><code>&lt;dataset&gt;_tkg/\n    entity_ids.txt\n    entity_id2_name.json\n    entity_id2_description.json\n    relation_id2name.json\n</code></pre>"},{"location":"standardised_tkg/#notes","title":"Notes","text":"<ul> <li>All files use UTF-8 encoding.</li> <li>JSON files are formatted for readability (indentation).</li> <li>The same format is used for all supported datasets, enabling unified loading and processing.</li> </ul>"},{"location":"standardised_tkg/#usage","title":"Usage","text":"<p>These files can be loaded directly using the provided Python loaders, or processed further for downstream tasks such as training text-based KGC models.</p>"},{"location":"wikidata5m_example/","title":"Tutorial: Processing Wikidata5M Data with TextKGCData CLI","text":"<p>This tutorial guides you through the complete workflow for processing the Wikidata5M dataset using the TextKGCData CLI, from downloading the raw data to loading the processed files for use in your code.</p>"},{"location":"wikidata5m_example/#1-download-the-wikidata5m-dataset","title":"1. Download the Wikidata5M Dataset","text":"<p>First, download the text-based KGC dataset (including Wikidata5M) from the SimKGC repository:</p> <pre><code>tkg download-text-kgc-dataset --data-dir-name wikidata5m\n</code></pre> <p>This will create a directory (default: <code>wikidata5m/</code>) with the raw data files.</p>"},{"location":"wikidata5m_example/#2-standardize-entity-files","title":"2. Standardize Entity Files","text":"<p>Convert the raw Wikidata5M entity files into standardized files for entity IDs, names, and descriptions:</p> <pre><code>tkg standardize-wikidata5m-entity-files-cli \\\n  --entity-names-source-path wikidata5m/wikidata5m_entity.txt \\\n  --entity-descriptions-source-path wikidata5m/wikidata5m_text.txt \\\n  --entity-id-save-path wikidata5m_tkg/entity_ids.txt \\\n  --entity-id2name-save-path wikidata5m_tkg/entity_id2_name.json \\\n  --entity-id2description-save-path wikidata5m_tkg/entity_id2_description.json\n</code></pre> <p>This will generate: - <code>wikidata5m_tkg/entity_ids.txt</code> - <code>wikidata5m_tkg/entity_id2_name.json</code> - <code>wikidata5m_tkg/entity_id2_description.json</code></p>"},{"location":"wikidata5m_example/#3-standardize-relation-file","title":"3. Standardize Relation File","text":"<p>Convert the raw Wikidata5M relations file into a standardized JSON mapping:</p> <pre><code>tkg standardize-wikidata5m-relation-file-cli \\\n  --relations-source-path wikidata5m/wikidata5m_relation.txt \\\n  --relation-id2name-save-path wikidata5m_tkg/relation_id2name.json\n</code></pre> <p>This will generate: - <code>wikidata5m_tkg/relation_id2name.json</code></p>"},{"location":"wikidata5m_example/#4-fill-missing-entity-namesdescriptions-optional","title":"4. Fill Missing Entity Names/Descriptions (Optional)","text":"<p>If you want to ensure that every entity has both a name and a description, fill missing entries with a placeholder:</p> <pre><code>tkg fill-missing-entries-cli \\\n  --entity-id2name-source-path wikidata5m_tkg/entity_id2_name.json \\\n  --entity-id2description-source-path wikidata5m_tkg/entity_id2_description.json \\\n  --entity-id2name-save-path wikidata5m_tkg/filled_entity_id2_name.json \\\n  --entity-id2description-save-path wikidata5m_tkg/filled_entity_id2_description.json \\\n  --place-holder-character \"-\"\n</code></pre>"},{"location":"wikidata5m_example/#5-truncate-descriptions-optional-for-model-compatibility","title":"5. Truncate Descriptions (Optional, for model compatibility)","text":"<p>To ensure descriptions fit within a model's token limit (e.g., 50 tokens for SimKGC), run:</p> <pre><code>tkg truncate-description-cli \\\n  --tokenizer-name bert-base-uncased \\\n  --entity-id2description-path wikidata5m_tkg/filled_entity_id2_description.json \\\n  --output-entity-id2description-path wikidata5m_tkg/truncated_entity_id2_description.json \\\n  --truncate-tokens 50\n</code></pre>"},{"location":"wikidata5m_example/#6-load-the-processed-data-in-python","title":"6. Load the Processed Data in Python","text":"<p>You can now load the processed Wikidata5M files using the <code>load_tkg_from_files</code> utility:</p> <pre><code>from deer_dataset_manager.tkg_io import load_tkg_from_files\n\nentity_id2name_source_path = \"wikidata5m_tkg/filled_entity_id2_name.json\"  # Dict[str, str]\nentity_id2description_source_path = \"wikidata5m_tkg/truncated_entity_id2_description.json\" # Dict[str, str]\nrelation_id2name_source_path = \"wikidata5m_tkg/relation_id2name.json\" # Dict[str, str]\n\ntextual_wikidata5m_kg = load_tkg_from_files(\n    entity_id2name_source_path,\n    entity_id2description_source_path,\n    relation_id2name_source_path,\n)\n</code></pre>"},{"location":"wikidata5m_example/#notes","title":"Notes","text":"<ul> <li>It picks the first name out of list of names provided in the original Wikidata5M files for the entity names as well as relation names.</li> <li>All CLI commands support custom input/output paths for flexible workflows.</li> <li>You can skip steps 4 and 5 if your data is already complete and within token limits.</li> <li>For more details, see the main documentation or run <code>tkg --help</code>.</li> </ul>"},{"location":"wn18rr_example/","title":"Tutorial: Processing WN18RR Data with TextKGCData CLI","text":"<p>This tutorial walks you through the complete workflow for processing the WN18RR dataset using the TextKGCData CLI, from downloading the raw data to loading the processed files for use in your code.</p>"},{"location":"wn18rr_example/#1-download-the-wn18rr-dataset","title":"1. Download the WN18RR Dataset","text":"<p>First, download the text-based KGC dataset (including WN18RR) from the SimKGC repository:</p> <pre><code>tkg download-text-kgc-dataset --data-dir-name WN18RR\n</code></pre> <p>This will create a directory (default: <code>WN18RR/</code>) with the raw data files.</p>"},{"location":"wn18rr_example/#2-standardize-entity-files","title":"2. Standardize Entity Files","text":"<p>Convert the raw WN18RR entity definitions into standardized files for entity IDs, names, and descriptions:</p> <pre><code>tkg standardize-wn18rr-entity-files-cli \\\n  --definitions-source-path WN18RR/wordnet-mlj12-definitions.txt \\\n  --entity-id-save-path wn18rr_tkg/entity_ids.txt \\\n  --entity-id2name-save-path wn18rr_tkg/entity_id2_name.txt \\\n  --entity-id2description-save-path wn18rr_tkg/entity_id2_description.txt\n</code></pre> <p>This will generate: - <code>wn18rr_tkg/entity_ids.txt</code> - <code>wn18rr_tkg/entity_id2_name.txt</code> - <code>wn18rr_tkg/entity_id2_description.txt</code></p>"},{"location":"wn18rr_example/#3-standardize-relation-file","title":"3. Standardize Relation File","text":"<p>Convert the raw WN18RR relations file into a standardized JSON mapping:</p> <pre><code>tkg standardize-wn18rr-relation-file-cli \\\n  --relations-source-path WN18RR/relations.dict \\\n  --relation-id2name-save-path wn18rr_tkg/wn18rr-relations2description.json\n</code></pre> <p>This will generate: - <code>wn18rr_tkg/wn18rr-relations2description.json</code></p>"},{"location":"wn18rr_example/#4-fill-missing-entity-namesdescriptions-optional","title":"4. Fill Missing Entity Names/Descriptions (Optional)","text":"<p>If you want to ensure that every entity has both a name and a description, fill missing entries with a placeholder:</p> <pre><code>tkg fill-missing-entries-cli \\\n  --entity-id2name-source-path wn18rr_tkg/entity_id2_name.txt \\\n  --entity-id2description-source-path wn18rr_tkg/entity_id2_description.txt \\\n  --entity-id2name-save-path wn18rr_tkg/filled_entity_id2_name.json \\\n  --entity-id2description-save-path wn18rr_tkg/filled_entity_id2_description.json \\\n  --place-holder-character \"-\"\n</code></pre>"},{"location":"wn18rr_example/#5-truncate-descriptions-optional-for-model-compatibility","title":"5. Truncate Descriptions (Optional, for model compatibility)","text":"<p>To ensure descriptions fit within a model's token limit (e.g., 50 tokens for SimKGC), run:</p> <pre><code>tkg truncate-description-cli \\\n  --tokenizer-name bert-base-uncased \\\n  --entity-id2description-path wn18rr_tkg/filled_entity_id2_description.json \\\n  --output-entity-id2description-path wn18rr_tkg/truncated_entity_id2_description.json \\\n  --truncate-tokens 50\n</code></pre>"},{"location":"wn18rr_example/#6-load-the-processed-data-in-python","title":"6. Load the Processed Data in Python","text":"<p>You can now load the processed WN18RR files using the <code>SimKGCDataLoader</code>:</p> <pre><code>from deer_dataset_manager.tkg_io import load_tkg_from_files\n\nentity_id2name_source_path = \"path/to/filled_entity_id2_name.json\"  # Dict[str, str]\nentity_id2description_source_path = \"wn18rr_tkg/truncated_entity_id2_description.json\" # Dict[str, str]\nrelation_id2name_source_path = \"path/to/relation_id2name.json\" # Dict[str, str]\n\ntextual_wn18rr_kg = load_tkg_from_files(\n    entity_id2name_source_path,\n    entity_id2description_source_path,\n    relation_id2name_source_path,\n)\n</code></pre>"},{"location":"wn18rr_example/#notes","title":"Notes","text":"<ul> <li>All CLI commands support custom input/output paths for flexible workflows.</li> <li>You can skip steps 4 and 5 if your data is already complete and within token limits.</li> <li>For more details, see the main documentation or run <code>tkg --help</code>.</li> </ul>"},{"location":"blog/","title":"Blog","text":""}]}